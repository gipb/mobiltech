{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "graph(%actual_input_1 : Float(10, 3, 224, 224)\n",
      "      %learned_0 : Float(64, 3, 11, 11)\n",
      "      %learned_1 : Float(64)\n",
      "      %learned_2 : Float(192, 64, 5, 5)\n",
      "      %learned_3 : Float(192)\n",
      "      %learned_4 : Float(384, 192, 3, 3)\n",
      "      %learned_5 : Float(384)\n",
      "      %learned_6 : Float(256, 384, 3, 3)\n",
      "      %learned_7 : Float(256)\n",
      "      %learned_8 : Float(256, 256, 3, 3)\n",
      "      %learned_9 : Float(256)\n",
      "      %learned_10 : Float(4096, 9216)\n",
      "      %learned_11 : Float(4096)\n",
      "      %learned_12 : Float(4096, 4096)\n",
      "      %learned_13 : Float(4096)\n",
      "      %learned_14 : Float(1000, 4096)\n",
      "      %learned_15 : Float(1000)) {\n",
      "  %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]\n",
      "  %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n",
      "  %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n",
      "  %20 : Float(10, 192, 27, 27) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%19, %learned_2, %learned_3), scope: AlexNet/Sequential[features]/Conv2d[3]\n",
      "  %21 : Float(10, 192, 27, 27) = onnx::Relu(%20), scope: AlexNet/Sequential[features]/ReLU[4]\n",
      "  %22 : Float(10, 192, 13, 13) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%21), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n",
      "  %23 : Float(10, 384, 13, 13) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%22, %learned_4, %learned_5), scope: AlexNet/Sequential[features]/Conv2d[6]\n",
      "  %24 : Float(10, 384, 13, 13) = onnx::Relu(%23), scope: AlexNet/Sequential[features]/ReLU[7]\n",
      "  %25 : Float(10, 256, 13, 13) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%24, %learned_6, %learned_7), scope: AlexNet/Sequential[features]/Conv2d[8]\n",
      "  %26 : Float(10, 256, 13, 13) = onnx::Relu(%25), scope: AlexNet/Sequential[features]/ReLU[9]\n",
      "  %27 : Float(10, 256, 13, 13) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%26, %learned_8, %learned_9), scope: AlexNet/Sequential[features]/Conv2d[10]\n",
      "  %28 : Float(10, 256, 13, 13) = onnx::Relu(%27), scope: AlexNet/Sequential[features]/ReLU[11]\n",
      "  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n",
      "  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet\n",
      "  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet\n",
      "  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\n",
      "  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\n",
      "  %34 : Dynamic = onnx::Unsqueeze[axes=[0]](%32), scope: AlexNet\n",
      "  %35 : Dynamic = onnx::Unsqueeze[axes=[0]](%33), scope: AlexNet\n",
      "  %36 : Dynamic = onnx::Concat[axis=0](%34, %35), scope: AlexNet\n",
      "  %37 : Float(10, 9216) = onnx::Reshape(%29, %36), scope: AlexNet\n",
      "  %38 : Float(10, 9216), %39 : Dynamic = onnx::Dropout[is_test=1, ratio=0.5](%37), scope: AlexNet/Sequential[classifier]/Dropout[0]\n",
      "  %40 : Float(10, 4096) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%38, %learned_10, %learned_11), scope: AlexNet/Sequential[classifier]/Linear[1]\n",
      "  %41 : Float(10, 4096) = onnx::Relu(%40), scope: AlexNet/Sequential[classifier]/ReLU[2]\n",
      "  %42 : Float(10, 4096), %43 : Dynamic = onnx::Dropout[is_test=1, ratio=0.5](%41), scope: AlexNet/Sequential[classifier]/Dropout[3]\n",
      "  %44 : Float(10, 4096) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%42, %learned_12, %learned_13), scope: AlexNet/Sequential[classifier]/Linear[4]\n",
      "  %45 : Float(10, 4096) = onnx::Relu(%44), scope: AlexNet/Sequential[classifier]/ReLU[5]\n",
      "  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]\n",
      "  return (%output1);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "\n",
    "dummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()\n",
    "model = torchvision.models.alexnet(pretrained=True).cuda()\n",
    "print(model)\n",
    "# Providing input and output names sets the display names for values\n",
    "# within the model's graph. Setting these does not change the semantics\n",
    "# of the graph; it is only for readability.\n",
    "#\n",
    "# The inputs to the network consist of the flat list of inputs (i.e.\n",
    "# the values you would pass to the forward() method) followed by the\n",
    "# flat list of parameters. You can partially specify names, i.e. provide\n",
    "# a list here shorter than the number of inputs to the model, and we will\n",
    "# only set that subset of names, starting from the beginning.\n",
    "input_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\n",
    "output_names = [ \"output1\" ]\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "23"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ONNX",
   "language": "python",
   "name": "onnx_practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
